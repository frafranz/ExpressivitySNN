#!/usr/bin/env python
# encoding: utf-8

# This currently just a collection of error cases disovered during debugging If
# this script completes without errors, it is an indication that things should
# be fiiiine..
# 
# TODO: turn in to actual unittests if desired.

# needs to be imported first
import torch

import lambertw_cuda as lwc
import numpy as np
import os.path as osp
import pylab as p
from scipy.special import lambertw

import traceback


def generate_data():
    # test that values slightly lower than the lower limit do not go into
    # infinite loops
    testdata = np.linspace(-1.0 / np.e * 1.05, 1.0, 100000)

    dev = torch.device("cuda")

    tensor = torch.from_numpy(testdata).data.to(dev)
    result = lwc.lambertw0(tensor)

    print(result)
    print(result.stride())
    return (testdata, result.cpu().detach().numpy())


def generate_data_multidim():
    testdata = np.linspace(-1.0 / np.e, 1.0, 1000).reshape(
        (20, 1, 5, -1), order="C"
    )

    dev = torch.device("cuda")

    tensor = torch.from_numpy(testdata).data.to(dev)
    result = lwc.lambertw0(tensor)

    print(result)
    print(testdata.shape, result.shape)
    print(result.stride())

    return (testdata, result.cpu().detach().numpy())


def get_filename(*paths):
    return osp.join(osp.dirname(osp.abspath(__file__)), *paths)


def infinite_loop():
    inputdata = torch.tensor(
        [
            [
                [
                    -5.1709e-01,
                    -1.1130e00,
                    -2.3675e00,
                    -4.3037e-01,
                    -8.1127e-01,
                    2.2121e00,
                    2.6516e00,
                    -1.3949e00,
                    -2.5663e00,
                    -8.9930e00,
                    -7.7420e-01,
                    -2.2606e00,
                    -8.2943e-01,
                    -1.1852e00,
                    5.1220e00,
                    -8.3779e-01,
                    -2.1788e00,
                    -5.1811e-01,
                    -4.5028e-01,
                    -9.8047e00,
                    -5.7825e-01,
                    -2.4622e00,
                    -6.6315e-01,
                    -5.8329e-01,
                    -7.6237e-01,
                    -7.1336e00,
                    -4.8776e-01,
                    -5.5690e-01,
                    -1.9846e-01,
                    -4.7302e-01,
                    -1.2380e00,
                    -8.0229e-01,
                    -6.8492e-01,
                    2.3707e01,
                    -1.1220e00,
                    -3.5560e-01,
                    -4.8975e-01,
                    -3.8432e-01,
                    -7.3017e-01,
                    -5.5939e-01,
                    -6.2002e-01,
                    -2.9739e01,
                    -3.5345e-01,
                    -3.1540e-01,
                    -6.6991e-01,
                    -3.2901e-01,
                    -4.6560e-01,
                    -4.9952e-01,
                    -5.2585e-01,
                    -3.4673e-01,
                    -6.4795e-01,
                    -3.3353e-01,
                    -3.0001e-01,
                    -1.4444e-01,
                    -3.2251e-01,
                    -3.9705e-01,
                    -3.3268e-01,
                    -4.7522e-01,
                    -8.3111e-01,
                    -5.8358e-01,
                    -3.0994e-01,
                    -2.6631e-01,
                    -3.5692e-01,
                    -3.9401e-01,
                    -2.9011e-01,
                    -2.6230e-01,
                    -7.2334e-01,
                    -3.1600e-01,
                    -3.2373e-01,
                    -4.3662e-01,
                    -2.8780e-01,
                    -2.9225e-01,
                    -4.3928e-01,
                    -3.2189e-01,
                    -2.1495e-01,
                    -4.4844e-01,
                    -4.1220e-01,
                    -3.7487e-01,
                    -1.8106e-01,
                    -3.9288e-01,
                    -4.3050e-01,
                    -3.3079e-01,
                    -4.6283e-01,
                    3.4671e00,
                    -6.5043e-01,
                    -3.1101e-01,
                    -3.3086e-01,
                    -3.9438e-01,
                    -3.5289e-01,
                    -3.4877e-01,
                    -2.7809e-01,
                    -4.2907e-01,
                    -3.7262e-01,
                    -2.4129e-01,
                    -2.7902e-02,
                    -3.3178e-01,
                    -2.8056e-01,
                    -5.1219e-01,
                    -3.1910e-01,
                    -2.0050e-01,
                    -3.6428e-01,
                    -2.3617e-01,
                    -3.7484e-01,
                    -1.8240e-01,
                    -3.0832e-01,
                    -2.7650e-01,
                    -4.0628e-01,
                    -2.4464e-01,
                    2.3997e02,
                    -3.0234e-01,
                    -2.0498e-01,
                    -2.6722e-01,
                    -2.9296e-01,
                    -4.0793e-01,
                    -2.9647e-01,
                    -1.7377e-01,
                    -2.3722e-01,
                    -3.8683e-01,
                    -2.4656e-01,
                    -4.3529e-01,
                    -2.3526e-01,
                    -2.1134e-01,
                    -2.6965e-01,
                    -3.6053e-01,
                    -1.8608e-01,
                    -2.6933e-01,
                    -1.6552e-01,
                    -2.4657e-01,
                    -1.6981e-01,
                    -2.2677e-01,
                    -2.8730e-01,
                    -3.3912e-01,
                    -1.5860e-01,
                    -3.9945e-01,
                    -2.3612e-01,
                    -1.6835e-01,
                    -2.0346e-01,
                    -2.8730e-01,
                    -2.8129e-01,
                    -2.8429e-01,
                    -1.3290e-01,
                    -2.6764e-01,
                    -2.3520e-01,
                    -1.4950e-01,
                    -2.2369e-01,
                    -2.0606e-01,
                    -1.4536e-01,
                    -2.4370e-01,
                    -3.9552e-01,
                    -2.0061e-01,
                    -3.7300e-01,
                    -1.5590e-01,
                    -1.8918e-01,
                    -1.5336e-01,
                    -1.6910e-01,
                    -2.2475e-01,
                    -2.1129e-01,
                    -1.2356e-01,
                    -3.1628e-01,
                    -2.3507e-01,
                    -1.5245e-01,
                    -1.4615e-01,
                    -2.1032e-01,
                    -2.2059e-01,
                    -1.9429e-01,
                    -1.1927e-01,
                    -1.9457e-01,
                    -1.9772e-01,
                    -1.2756e-01,
                    -2.1034e-01,
                    -1.7499e-01,
                    -1.2437e-01,
                    -1.5795e-01,
                    -3.1679e-01,
                    -1.7579e-01,
                    -3.0203e-01,
                    -1.5721e-01,
                    -1.1680e-01,
                    -1.3054e-01,
                    -1.4907e-01,
                    -1.9199e-01,
                    -2.0999e-01,
                    -1.1205e-01,
                    -3.0605e-01,
                    -1.8378e-01,
                    -1.5501e-01,
                    -1.2659e-01,
                    -1.6914e-01,
                    -1.9632e-01,
                    -1.5191e-01,
                    -1.0679e-01,
                    -1.6673e-01,
                    -1.5955e-01,
                    -1.0946e-01,
                    -1.7801e-01,
                    -1.5038e-01,
                    -1.2076e-01,
                    -1.4664e-01,
                    -2.5762e-01,
                    -1.3685e-01,
                    -2.6476e-01,
                    -1.4491e-01,
                    -1.1608e-01,
                    -1.1835e-01,
                    -1.3926e-01,
                    -1.7546e-01,
                    -1.9374e-01,
                    -9.6212e-02,
                    -2.4416e-01,
                    -1.3753e-01,
                    -1.1427e-01,
                    -1.2162e-01,
                    -1.6397e-01,
                    -1.6462e-01,
                    -1.5585e-01,
                    -8.9267e-02,
                    -1.4415e-01,
                    -1.2262e-01,
                    -1.0526e-01,
                    -1.6569e-01,
                    -1.3231e-01,
                    -1.1488e-01,
                    -1.0431e-01,
                    -1.8885e-01,
                    -1.4207e-01,
                ]
            ],
            [
                [
                    9.5347e-01,
                    -2.6415e00,
                    -7.7482e-01,
                    -1.1736e00,
                    -6.4588e-01,
                    -9.9824e-01,
                    -5.2058e-01,
                    -5.5651e-01,
                    -1.5016e00,
                    -5.2508e01,
                    -1.5847e00,
                    -5.0105e-01,
                    -7.5263e-01,
                    -9.8971e-01,
                    -5.7046e-01,
                    -1.1450e00,
                    -7.0878e-01,
                    -1.1920e00,
                    -8.5930e-01,
                    -3.4651e00,
                    -1.1191e00,
                    -8.4977e-01,
                    -4.4312e-01,
                    -7.7016e-01,
                    -1.2967e00,
                    -2.8222e00,
                    -7.5379e-01,
                    -3.5359e-01,
                    -3.6540e-01,
                    -3.9449e-01,
                    -3.6865e-01,
                    -2.7171e-01,
                    -4.0964e-01,
                    -5.2319e-01,
                    -1.1886e00,
                    -9.5671e-01,
                    -2.6962e-01,
                    -6.5427e-01,
                    -4.5895e-01,
                    -2.9306e-01,
                    -3.2542e-01,
                    -3.6236e-01,
                    -8.5167e-01,
                    -9.2408e-01,
                    -9.2064e-01,
                    -7.5257e-01,
                    -4.0803e-01,
                    -3.9506e-01,
                    -3.9946e-01,
                    -3.9382e-01,
                    -1.0037e00,
                    -7.8584e-01,
                    -1.6234e-01,
                    -2.4644e-01,
                    -2.9919e-01,
                    -2.8681e-01,
                    -2.6950e-01,
                    -3.0534e-01,
                    -4.9556e-01,
                    -4.9079e-01,
                    -1.0691e00,
                    -2.0895e-01,
                    -3.6911e-01,
                    -3.6357e-01,
                    -2.0461e-01,
                    -2.4602e-01,
                    -2.7617e-01,
                    -4.1448e-01,
                    -4.1869e-01,
                    -5.1005e-01,
                    -4.3741e-01,
                    -3.7116e-01,
                    -3.3078e-01,
                    -3.0274e-01,
                    -2.3595e-01,
                    -4.1638e-01,
                    -6.4984e-01,
                    -1.9823e-01,
                    -2.2663e-01,
                    -3.2714e-01,
                    -2.7798e-04,
                    -2.5958e-02,
                    -3.7457e-01,
                    -6.1653e-01,
                    -5.8673e-01,
                    -5.7528e-01,
                    -2.6100e-01,
                    -3.7806e-01,
                    -4.1856e-01,
                    -1.4612e-01,
                    -2.8619e-01,
                    -3.4912e-01,
                    -3.1950e-01,
                    -2.9470e-01,
                    -6.0696e-01,
                    -3.4817e-01,
                    -4.6860e-01,
                    -3.2361e-01,
                    -2.9114e-01,
                    -2.7103e-01,
                    -2.9605e-01,
                    -2.9849e-01,
                    -1.7952e-01,
                    -1.8350e-01,
                    -2.4024e-01,
                    8.4347e10,
                    -3.3342e-01,
                    -2.1884e-01,
                    -2.7874e-01,
                    -4.7314e-01,
                    -3.5495e-01,
                    -2.2983e-01,
                    -3.6954e-01,
                    -2.9157e-01,
                    -1.9855e-01,
                    -1.9823e-01,
                    -2.6594e-01,
                    -1.9783e-01,
                    -1.6539e-01,
                    -2.9987e-01,
                    -2.8735e-01,
                    -2.5424e-01,
                    -2.8971e-01,
                    -3.5904e-01,
                    -2.9128e-01,
                    -2.0630e-01,
                    -2.5230e-01,
                    -1.7199e-01,
                    -1.6391e-01,
                    -2.2952e-01,
                    np.inf,
                    -2.7046e-01,
                    -1.7875e-01,
                    -3.4790e-01,
                    -3.7635e-01,
                    -2.3123e-01,
                    -2.0153e-01,
                    -2.7642e-01,
                    -2.0381e-01,
                    -2.5696e-01,
                    -1.5338e-01,
                    -2.8089e-01,
                    -1.9222e-01,
                    -1.1565e-01,
                    -3.2281e-01,
                    -2.2153e-01,
                    -1.7626e-01,
                    -2.4571e-01,
                    -3.5944e-01,
                    -1.7957e-01,
                    -3.2680e-01,
                    -1.9569e-01,
                    -1.4266e-01,
                    -1.1422e-01,
                    -1.9229e-01,
                    -3.5509e-01,
                    -1.9274e-01,
                    -1.5820e-01,
                    -4.0551e-01,
                    -2.9353e-01,
                    -1.7116e-01,
                    -1.5543e-01,
                    -2.0130e-01,
                    -1.8444e-01,
                    -1.8759e-01,
                    -1.4436e-01,
                    -3.1046e-01,
                    -1.6417e-01,
                    -1.0424e-01,
                    -2.2416e-01,
                    -1.7219e-01,
                    -1.3520e-01,
                    -2.2014e-01,
                    -3.4378e-01,
                    -1.4123e-01,
                    -2.8334e-01,
                    -1.7681e-01,
                    -1.4166e-01,
                    -1.0413e-01,
                    -1.7652e-01,
                    -3.1972e-01,
                    -1.7913e-01,
                    -1.2865e-01,
                    -3.0661e-01,
                    -1.9175e-01,
                    -1.2243e-01,
                    -1.4825e-01,
                    -1.9406e-01,
                    -1.5620e-01,
                    -1.9308e-01,
                    -1.1435e-01,
                    -2.4922e-01,
                    -1.2517e-01,
                    -1.0042e-01,
                    -2.0499e-01,
                    -1.4874e-01,
                    -1.2786e-01,
                    -1.3762e-01,
                    -2.3865e-01,
                    -1.4684e-01,
                    -2.4639e-01,
                    -1.2828e-01,
                    -1.3627e-01,
                    -1.0264e-01,
                    -1.4161e-01,
                    -1.9071e-01,
                    -2.0601e-01,
                    -1.0223e-01,
                    -2.7103e-01,
                    -1.4082e-01,
                    -1.0040e-01,
                    -1.2930e-01,
                    -1.6275e-01,
                    -1.6751e-01,
                    -1.7248e-01,
                    -9.0303e-02,
                    -1.7187e-01,
                    -1.1708e-01,
                    -1.0137e-01,
                    -1.6118e-01,
                    -1.2253e-01,
                    -1.1017e-01,
                    -1.0873e-01,
                    -1.9209e-01,
                    -1.3839e-01,
                ]
            ],
        ],
        device="cuda:0",
    )
    test_data_one_by_one(inputdata)
    result = lwc.lambertw0(inputdata)

    return inputdata, result


def plot_data(data):
    x, f_x = data

    fig = p.figure()
    ax = fig.add_subplot()

    ax.plot(x, f_x - lambertw(x))
    ax.set_ylabel(r"$\Delta (W_{cuda} - W_{scipy})$ ")
    ax.set_xlabel("z [1]")

    fig.savefig("diff_lambertw.svg")

    p.show()


def real_data():
    """
    Test if real data computes.
    """
    inputdata = torch.tensor(
        [
            [
                [
                    -1.0869,
                    -1.8327,
                    -1.2424,
                    -1.0942,
                    -0.9991,
                    -0.8877,
                    -1.1161,
                    -1.2724,
                    -1.6540,
                    -1.1425,
                    -0.6048,
                    -0.9116,
                    -0.5762,
                    -0.6628,
                    -0.5637,
                    -0.5615,
                    -0.6271,
                    -0.7392,
                    -0.6238,
                    -0.6722,
                    -0.3661,
                    -0.6136,
                    -0.3531,
                    -0.4202,
                    -0.3970,
                    -0.3506,
                    -0.4279,
                    -0.4661,
                    -0.4024,
                    -0.4221,
                    -0.2929,
                    -0.3760,
                    -0.2537,
                    -0.2927,
                    -0.2643,
                    -0.2742,
                    -0.3197,
                    -0.3504,
                    -0.3096,
                    -0.2863,
                ]
            ],
            [
                [
                    -0.9072,
                    -1.8428,
                    -0.8995,
                    -1.1145,
                    -1.3055,
                    -0.9032,
                    -1.3119,
                    -1.2293,
                    -1.1241,
                    -1.1041,
                    -0.5527,
                    -0.9136,
                    -0.5016,
                    -0.6692,
                    -0.6346,
                    -0.5668,
                    -0.6750,
                    -0.7265,
                    -0.5441,
                    -0.6605,
                    -0.3704,
                    -0.6134,
                    -0.3602,
                    -0.4197,
                    -0.3905,
                    -0.3502,
                    -0.4236,
                    -0.4672,
                    -0.4114,
                    -0.4230,
                    -0.2959,
                    -0.3760,
                    -0.2577,
                    -0.2924,
                    -0.2610,
                    -0.2740,
                    -0.3170,
                    -0.3511,
                    -0.3154,
                    -0.2868,
                ]
            ],
            [
                [
                    -1.4516,
                    -0.9652,
                    -0.8961,
                    -0.9550,
                    -0.7830,
                    -1.2435,
                    -1.2507,
                    -1.3972,
                    -1.3371,
                    -0.8825,
                    -0.6855,
                    -0.6583,
                    -0.5007,
                    -0.6157,
                    -0.4971,
                    -0.6646,
                    -0.6610,
                    -0.7734,
                    -0.5813,
                    -0.5843,
                    -0.4227,
                    -0.4911,
                    -0.3597,
                    -0.3992,
                    -0.3360,
                    -0.3831,
                    -0.4183,
                    -0.4852,
                    -0.4318,
                    -0.3921,
                    -0.2888,
                    -0.3897,
                    -0.2578,
                    -0.2950,
                    -0.2681,
                    -0.2695,
                    -0.3178,
                    -0.3487,
                    -0.3125,
                    -0.2905,
                ]
            ],
            [
                [
                    -1.0989,
                    -1.5138,
                    -0.9130,
                    -1.3096,
                    -1.0370,
                    -1.1662,
                    -1.1493,
                    -1.3985,
                    -0.8866,
                    -1.2878,
                    -0.6968,
                    -0.6416,
                    -0.5001,
                    -0.6044,
                    -0.4889,
                    -0.6672,
                    -0.6643,
                    -0.7734,
                    -0.5951,
                    -0.5717,
                    -0.4300,
                    -0.4770,
                    -0.3593,
                    -0.3914,
                    -0.3300,
                    -0.3846,
                    -0.4205,
                    -0.4851,
                    -0.4435,
                    -0.3828,
                    -0.2927,
                    -0.3795,
                    -0.2575,
                    -0.2901,
                    -0.2638,
                    -0.2703,
                    -0.3192,
                    -0.3487,
                    -0.3199,
                    -0.2847,
                ]
            ],
        ],
        device="cuda:0",
    )

    test_data_one_by_one(inputdata)

    result = lwc.lambertw0(inputdata)

    print(result)
    print(result.stride())
    return (inputdata, result.cpu().detach().numpy())


def find_limits_float64():
    #  data = np.power(10, np.linspace(0, 306, 1000))
    #  data = np.power(10, np.linspace(305, 306, 1000))
    data = np.linspace(6.4686e+305, 6.4835e+305, 10000)

    test_data_one_by_one(data)


def find_limits_float32():
    #  data = np.power(10, np.linspace(0, 306, 1000))
    #  data = np.power(10, np.linspace(305, 306, 1000))
    data = np.linspace(5.7116e+36, 1.1563e+37, 10000)

    test_data_one_by_one(data, dtype=torch.float32)


def test_data_one_by_one(data, dtype=None):
    if dtype is None:
        dtype = torch.float64

    for d in data:
        t = torch.tensor([d], dtype=dtype).cuda()
        print(f"Trying: {t}")
        result = lwc.lambertw0(t)
        print(f"Result: {d} -> {result}")
    traceback.print_stack()


def test_file_one_by_one(filename):
    test_data_one_by_one(np.loadtxt(get_filename(filename)))


def test_tmp_data():
    try:
        data = np.loadtxt("/tmp/zForLambertW.txt")
    except IOError:
        return None

    data[np.abs(data - -1 / np.e) < 3.5e-3] = 0.0

    test_data_one_by_one(data)

    data = torch.from_numpy(data).cuda()
    print(data)
    result = lwc.lambertw0(data)

    return data, result


if __name__ == "__main__":
    find_limits_float32()
    find_limits_float64()

    #  test_file_one_by_one("faulty_input_02.txt")
    #  test_file_one_by_one("faulty_input_01.txt")

    #  print(test_tmp_data())

    #  data = generate_data()
    #  #  plot_data(data)

    #  multi_data, multi_result = generate_data_multidim()
    #  #  plot_data((multi_data.flatten(), multi_result.flatten()))

    #  print(real_data())

    #  print(infinite_loop())
